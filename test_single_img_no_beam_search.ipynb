{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/isl/anaconda3/envs/SAI/lib/python3.9/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/isl/anaconda3/envs/SAI/lib/python3.9/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim\n",
    "import torch.utils.data\n",
    "import torchvision.transforms as transforms\n",
    "from torch import nn\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "from models import Encoder, DecoderWithAttention\n",
    "from datasets import *\n",
    "from utils import *\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import numpy as np\n",
    "import skimage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model parameters\n",
    "emb_dim = 512  # dimension of word embeddings\n",
    "attention_dim = 512  # dimension of attention linear layers\n",
    "decoder_dim = 512  # dimension of decoder RNN\n",
    "data_folder = 'path_to_output_folder'  # folder with data files saved by create_input_files.py\n",
    "data_name = 'flickr8k_5_cap_per_img_5_min_word_freq'  # base name shared by data files\n",
    "dataset = \"flickr8k\"\n",
    "output_folder = \"path_to_output_folder\"\n",
    "captions_per_image = 5\n",
    "min_word_freq = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_alphas(image_path, caption, alphas, smooth=True):\n",
    "    \"\"\"\n",
    "    Visualizes caption with weights at every word.\n",
    "\n",
    "    Adapted from paper authors' repo: https://github.com/kelvinxu/arctic-captions/blob/master/alpha_visualization.ipynb\n",
    "\n",
    "    :param image_path: path to image that has been captioned\n",
    "    :param caption: caption\n",
    "    :param alphas: weights\n",
    "    :param smooth: smooth weights?\n",
    "    \"\"\"\n",
    "    image = Image.open(image_path)\n",
    "    image = image.resize([14 * 16, 14 * 16], Image.LANCZOS)\n",
    "\n",
    "    print(int(np.ceil((len(caption)/5.0))), len(caption))\n",
    "    plt.subplot(int(np.ceil((len(caption))/5.0)), 5, 1)\n",
    "\n",
    "    plt.text(0, 1, '%s' % (caption[0]), color='black', backgroundcolor='white', fontsize=8)\n",
    "    plt.imshow(image)\n",
    "    for t in range(1,len(caption)):\n",
    "        if t > 50:\n",
    "            break\n",
    "        plt.subplot(int(np.ceil(len(caption)/5.0)), 5, t + 1)\n",
    "        plt.text(0, 1, '%s' % (caption[t]), color='black', backgroundcolor='white', fontsize=8)\n",
    "        plt.imshow(image)\n",
    "        current_alpha = alphas[t-1, :]\n",
    "        if smooth:\n",
    "            alpha = skimage.transform.pyramid_expand(current_alpha.cpu().detach().numpy(), upscale=16, sigma=8)\n",
    "        else:\n",
    "            alpha = skimage.transform.resize(current_alpha.cpu().detach().numpy(), [14 * 16, 14 * 16])\n",
    "        if t == 0:\n",
    "            plt.imshow(alpha, alpha=0)\n",
    "        else:\n",
    "            plt.imshow(alpha, alpha=0.8)\n",
    "\n",
    "        plt.set_cmap(cm.Greys_r)\n",
    "        plt.axis('off')\n",
    "    plt.savefig('path_to_predictions/alphas_img_{}.jpg'.format(3))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(img_path, img):\n",
    "    word_map_file = os.path.join(data_folder, 'WORDMAP_' + data_name + '.json')\n",
    "    with open(word_map_file, 'r') as j:\n",
    "        word_map = json.load(j)\n",
    "\n",
    "    index_map = {index: word for word, index in word_map.items()}\n",
    "    # Create a base/root name for all output files\n",
    "    base_filename = dataset + '_' + str(captions_per_image) + '_cap_per_img_' + str(min_word_freq) + '_min_word_freq'\n",
    "\n",
    "    # Save word map to a JSON\n",
    "    with open(os.path.join(output_folder, 'INDEXMAP_' + base_filename + '.json'), 'w') as j:\n",
    "        json.dump(index_map, j)\n",
    "\n",
    "    index_map_file = os.path.join(data_folder, 'INDEXMAP_' + data_name + '.json')\n",
    "    with open(index_map_file, 'r') as j:\n",
    "        index_map = json.load(j)\n",
    "\n",
    "    checkpoint =  torch.load(\"./BEST_checkpoint_flickr8k_5_cap_per_img_5_min_word_freq.pth.tar\")\n",
    "    decoder = checkpoint['decoder']\n",
    "    encoder = checkpoint['encoder']\n",
    "    #encoder = torch.load(\"flickr8k/trained_models/encoder\")\n",
    "    #decoder = torch.load(\"flickr8k/trained_models/decoder\")\n",
    "\n",
    "    decoder = decoder\n",
    "    encoder = encoder\n",
    "    img = img # (3,224,224)\n",
    "\n",
    "    # Encoder\n",
    "    encoder_out = encoder(img.unsqueeze(0)) # (1,enc_img_size,enc_img_size,encoder_dim)\n",
    "\n",
    "\n",
    "    # Decoder\n",
    "    batch_size = encoder_out.size(0)\n",
    "    encoder_dim = encoder_out.size(-1)\n",
    "    enc_img_size = encoder_out.size(1)\n",
    "    vocab_size = len(word_map)\n",
    "\n",
    "    # Flatten image\n",
    "    encoder_out = encoder_out.view(batch_size, -1, encoder_dim)  # (batch_size, num_pixels, encoder_dim)\n",
    "    num_pixels = encoder_out.size(1)\n",
    "\n",
    "    caption = [\"<start>\"]\n",
    "    last_word = word_map['<start>']\n",
    "\n",
    "    # Embedding\n",
    "    embeddings = decoder.embedding(torch.tensor([last_word])) # (1,embed_dim)\n",
    "\n",
    "    # Initialize LSTM state\n",
    "    h, c = decoder.init_hidden_state(encoder_out)  # (1, decoder_dim)\n",
    "    alphas = []\n",
    "    while(last_word != word_map[\"<end>\"]):\n",
    "        attention_weighted_encoding, alpha = decoder.attention(encoder_out,h)\n",
    "        # awe.shape = (1,encoder_dim)\n",
    "        # alpha.shape = (1,num_pixels(14*14)) \n",
    "        alphas.append(alpha.view(enc_img_size,enc_img_size))\n",
    "        gate = decoder.sigmoid(decoder.f_beta(h))  # gating scalar, (1, encoder_dim)\n",
    "        attention_weighted_encoding = gate * attention_weighted_encoding\n",
    "\n",
    "        h, c = decoder.decode_step(\n",
    "                torch.cat([embeddings, attention_weighted_encoding], dim=1), # (1, embed_dim+encoder_fim)\n",
    "                (h, c))\n",
    "        \n",
    "        preds = decoder.fc(decoder.dropout(h))  # (1, vocab_size)\n",
    "        pred = torch.argmax(preds).item()\n",
    "        last_word = pred\n",
    "        caption.append(index_map[str(pred)])\n",
    "        embeddings = decoder.embedding(torch.tensor([last_word])) # (1,embed_dim)\n",
    "\n",
    "\n",
    "    alphas = torch.stack(alphas)\n",
    "    print(\"alphas_shape : \",alphas.shape)\n",
    "    visualize_alphas(img_path,caption, alphas)\n",
    "    return ' '.join(caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(img_path):\n",
    "    img = Image.open(img_path)\n",
    "    #plt.imshow(np.array(img))\n",
    "    #plt.show()\n",
    "    #img.show()\n",
    "    img = img.resize((256, 256))\n",
    "    img = np.array(img)\n",
    "    img = img.transpose(2, 0, 1)\n",
    "    assert img.shape == (3, 256, 256)\n",
    "    assert np.max(img) <= 255\n",
    "\n",
    "    # Add random noise to the image array for probabilistic captions\n",
    "    noise = np.random.normal(loc=0, scale=1, size=img.shape).astype(np.uint8)\n",
    "    img = np.clip(img + noise, 0, 255)\n",
    "\n",
    "\n",
    "    transform=transforms.Compose([transforms.Normalize(\n",
    "                                    mean=[0.485, 0.456, 0.406],\n",
    "                                    std=[0.229, 0.224, 0.225])])\n",
    "    img = torch.FloatTensor(img/ 255.)\n",
    "    img = transform(img)\n",
    "\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "img_path = \"path_to_test_image\"\n",
    "img = process(img_path)\n",
    "print(test(img_path,img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
